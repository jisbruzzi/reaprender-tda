---
title: Programación Dinámica
módulos:
  - 6.1
  - 6.2
  - 6.3
  - 6.4
  - 6.6
---

# Weighted interval scheduling
## Planteo
Si se tienen $n$ trabajos donde:
- $s_i$: **inicio** del trabajo $i$
- $f_i$: **fin** del trabajo $i$
- $v_i$: **peso** del trabajo $i$

Dos trabajos son compatibles si no se superponen en el tiempo.

## Objetivo
Conjunto $S \subset \{ 1, ..., n \}$ de trabajos mutuamente compatibles que maximice $\sum_{i \in S} v_i$ .


## Construcción del algoritmo

### 1. Etiquetado
Supongamos que los intervalos $i$ están etiquetados de forma que: $f_1 \leq f_2 \leq ... \leq f_n$.

### 2. Definición de $p(j)$
Sea $p(j)$ el mayor $i<j$ tales que los intervalos $i$ y $j$ son compatibles. $p(j)$ podría pensarse intuitivamente como el intervalo inmediatamente previo al $j$.

![](img/4.weighted.interval.pj.png)

### 3. Definición de $\operatorname{opt}(j)$
$\operatorname{opt}(j)$ es el óptimo si el problema no fuera hasta $n$ sino hasta $j$

$$
\operatorname{opt}(j)=\max_{S \subset \{1,...,j\}}\sum_{i \in S} v_i
$$

El objetivo del Weighted interval scheduling podría definirse como: "Encontrar el conjunto $S \subset \{ 1, ..., n \}$ de trabajos mutuamente compatibles que maximice $\operatorname{opt}(n)$".

### 4. Definición recursiva de $\operatorname{opt}(j)$

El trabajo $j$ puede pertenecer o no al subconjunto de trabajos que alcanza $\operatorname{opt}(j)$.
- Si el trabajo $j$ **pertenece**, entonces $\operatorname{opt}(j)=v_j + \operatorname{opt}(p(j))$ (ya que todos los trabajos entre $j$ y $p(j)$ son incompatibles con el trabajo $j$)
- Si el trabajo $j$ **no pertenece**, entonces $\operatorname{opt}(j)=\operatorname{opt}(j-1)$

Por lo tanto, podemos formular $\operatorname{opt}(j)$ como:

$$
\operatorname{opt}(j)=\max(v_j + \operatorname{opt}(p(j)), \operatorname{opt}(j-1) )
$$

### 5. Exponencialidad del algoritmo recursivo

![](img/4.weighted.complexity.png)

Suponiendo un problema en el cual $p(j)=j-2 \forall j\in S$, las llamadas a la función $\operatorname{opt}$ para conseguir $\operatorname{opt}(j)$  son $\operatorname{calls}(j)$, que es: 

$$
\operatorname{calls}(j)= 2 + \operatorname{calls}(j-1) + \operatorname{calls}(j-2) \text{ si } j \gt 0
$$

$$
\operatorname{calls}(j)=0 \text{ si } j \leq 0
$$

Resolver la relación de recurrencia nos revela que $\operatorname{calls}(n)$ tiene una componente exponencial, con lo cual un algoritmo que calcule $\operatorname{opt}(n)$ usando valores anteriores pertenece a $O(k^n)$ (orden exponencial).

### 6. Memoización

$\operatorname{opt}(j)$ depende unicamente de $j$ con lo cual puede calcularse una sola vez y almacenarse, reduciendo la complejidad del algoritmo a $O(n)$.

### 7. Algoritmo memoizado

![](img/4.weighted.intervals.algoritmo.png)

Con una leve alteración se puede calcular la solución $S$ en vez del óptimo $\operatorname{opt}(n)$.

### 8. Algoritmo iterativo
En vez de usar una invocación recursiva se puede iterar hasta $n$ para calcular $\operatorname{opt}(n)$.

![](img/4.weighted.intervals.algoritmo.iterativo.png)

# Diseño de algoritmos dinámicos

El problema se resuelve por porgramación dinámica si cumple:

![](img/4.guidelines.png)

# Segmented Least Squares

## Planteo
Conjunto de puntos $P$, $|P|=n$

$$
P=\{(x_1,y_1),...,(x_n,y_n)\}
$$

donde $x_1 \lt ... \lt x_n$ y $p_i = (x_i,y_i)$

Recta de aproximación $L$, representa la recta $y=ax+b$:

$$
L=(a,b) \in \R^2
$$

Error cuadrático:

$$
\operatorname{Error}(L,P)=\sum_{\forall (x_P,y_P) \in P} (y_P-ax_P-b)^2
$$

La recta $L$ se puede calcular usando esta expresión:

![](img/4.expresion.recta.png)

Entonces vamos a hablar de $\operatorname{Error}(P)$, ya que dado $P$ podemos calcular su $L$ y su $\operatorname{Error}(L,P)$.


Sea la penalidad o _penalty_ de una solución (En Machine Learning se suele usar el término _loss_), siendo $S$ un conjunto de conjuntos de puntos contíguos, y $C$ una constante que usamos para penalizar a cada recta.

$$
\operatorname{opt}(P)=\min_{S \text{ particiona } P}\sum_{s \in S}(\operatorname{Error}(s) + C)
$$

La idea es encontrar varias rectas que aproximen un conjunto de puntos. Al algoritmo le gustaría poner $n$ rectas, así que penalizamos con la constante $C$.


## Objetivo
Encontrar una partición $S$ de $P$ en la que los puntos de las partes sean contíguos y que minimice $\operatorname{opt}(S)$.

## Planteo recursivo
$$
e_{i,j}=\operatorname{Error}(\{p_i,...,p_j\})
$$

$$
\operatorname{opt}(j)=\min_{i \in \{1 ... j\}} (e_{i,j}+C+\operatorname{opt}(i-1))
$$

Nótese que aquí se minimiza sobre un conjunto de $j$ elementos, no 2 como antes.

## Algoritmo

![](img/4.segmented.algoritmo.png)

# Mochila
## Planteo
Mochila con capacidad $W$, cada uno de los $n$ artículos ocupa $w_i$. Esto es, siendo $S$ el conjunto de artículos seleccionados:

$$
\sum_{i \in S} w_i \leq W
$$

## Objetivo
Maximizar la capacidad ocupada de la mochila. Es decir, hallar un $S$ que maximice:

$$
\max_S \sum_{i \in S} w_i
$$

## Planteo recurrente
Los subproblemas se plantean en términos de dos variables:  $i$ y $w$.

$$
\operatorname{opt}(i,w)=\max_{S \subset \{1, ..., i\}} \sum_{j\in S} w_j \text{ sujeto a } \sum_{j \in S} w_i \leq w
$$

Sea $O$ la solución óptima.
- Si $i \notin O$, se cumple $\operatorname{opt}(i,w) = \operatorname{opt}(i-1,w)$ porque $i$ no aporta al máximo
- Si $i \in O$, se cumple $\operatorname{opt}(i,w) = w_i + \operatorname{opt}(i-1,w-w_i)$ porque $i$ aporta al máximo y resta capacidad disponible

Por lo tanto:

$$
\operatorname{opt}(i,w)=\max(\operatorname{opt}(i-1,w),w_i + \operatorname{opt}(i-1,w-w_i))
$$

## Algoritmo

![](img/4.mochila.algoritmo.png)

El algoritmo pertenece a  $O(nW)$

## Planteo para el problema de la mochila posta

Donde los artículos tienen un valor $v_i$:

$$
\operatorname{opt}(i,w)=\max(\operatorname{opt}(i-1,w),v_i + \operatorname{opt}(i-1,w-w_i))
$$

# Sequence alignment
PENDIENTE